{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Processor: Pre-Processing & Representation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Processor Class Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AuthorshipData:\n",
    "\n",
    "    def __init__(self, train_file, test_file):\n",
    "\n",
    "        # Read training data & testing data\n",
    "        self.train_data = self.read_data(train_file)\n",
    "        self.test_data = self.read_data(test_file)\n",
    "\n",
    "        # Initially process data in one-hot encoding\n",
    "        self.label = self.process_label(self.train_data)\n",
    "\n",
    "        self.feature_train = self.process_feature(self.train_data)\n",
    "        self.feature_test = self.process_feature(self.test_data)\n",
    "\n",
    "        self.f_coauthor_tn, self.f_text_tn, self.f_year_tn, self.f_venue_tn = self.feature_train\n",
    "        self.f_coauthor_ts, self.f_text_ts, self.f_year_ts, self.f_venue_ts = self.feature_test\n",
    "\n",
    "    # Read data in initialization\n",
    "    def read_data(self, file_name):\n",
    "        with open(file_name, 'r', encoding='utf8') as data:\n",
    "            json_data = json.load(data)\n",
    "        output_data = pd.DataFrame(json_data)\n",
    "        return output_data\n",
    "\n",
    "    # Process data in one-hot in initialization\n",
    "    def process_label(self, data):\n",
    "\n",
    "        n_author = 101\n",
    "        threshold_author = 100\n",
    "\n",
    "        ath_list = []\n",
    "\n",
    "        for authors in data[\"authors\"]:\n",
    "\n",
    "            authors = np.array(authors)\n",
    "            ath_id = authors[authors < threshold_author]\n",
    "\n",
    "            ath_vector = np.zeros(n_author)\n",
    "            if len(ath_id) == 0:\n",
    "                ath_vector[-1] = 1\n",
    "            else:\n",
    "                ath_vector[ath_id] = 1\n",
    "            ath_list.append(ath_vector)\n",
    "\n",
    "        ath_list = np.array(ath_list)\n",
    "\n",
    "        return ath_list\n",
    "\n",
    "    def process_feature(self, data):\n",
    "\n",
    "        # Coauthor\n",
    "        if \"authors\" in data.columns:\n",
    "            author_column = \"authors\"\n",
    "        else:\n",
    "            author_column = \"coauthors\"\n",
    "\n",
    "        n_coauthor = 21247\n",
    "\n",
    "        cth_list = []\n",
    "\n",
    "        for authors in data[author_column]:\n",
    "\n",
    "            authors = np.array(authors)\n",
    "            cth_id = authors\n",
    "\n",
    "            cth_vector = np.zeros(n_coauthor)\n",
    "            if len(cth_id) == 0:\n",
    "                cth_vector[-1] = 1\n",
    "            else:\n",
    "                cth_vector[cth_id] = 1\n",
    "\n",
    "            cth_list.append(cth_vector)\n",
    "\n",
    "        cth_list = np.array(cth_list)\n",
    "\n",
    "        # Title & Abstract\n",
    "        n_text = 5000\n",
    "\n",
    "        text_list = []\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            title = data[\"title\"][i]\n",
    "            abstract = data[\"abstract\"][i]\n",
    "            text = title + abstract\n",
    "\n",
    "            text_vector = np.zeros(n_text)\n",
    "            text_vector[text] = 1\n",
    "            text_list.append(text_vector)\n",
    "\n",
    "        text_list = np.array(text_list)\n",
    "\n",
    "        # Year\n",
    "        n_year = 20\n",
    "        year_list = []\n",
    "\n",
    "        for year in data[\"year\"]:\n",
    "            year_vector = np.zeros(n_year)\n",
    "            year_vector[year] = 1\n",
    "            year_list.append(year_vector)\n",
    "\n",
    "        year_list = np.array(year_list)\n",
    "\n",
    "        # Vneue\n",
    "        n_venue = 466\n",
    "        ven_list = []\n",
    "\n",
    "        for venue in data[\"venue\"]:\n",
    "            ven_vector = np.zeros(n_venue)\n",
    "            if venue == \"\":\n",
    "                ven_vector[-1] = 1\n",
    "            else:\n",
    "                ven_vector[venue] = 1\n",
    "            ven_list.append(ven_vector)\n",
    "\n",
    "        ven_list = np.array(ven_list)\n",
    "\n",
    "        return cth_list, text_list, year_list, ven_list\n",
    "\n",
    "    # Extract data\n",
    "    def extract_data(self, extend_factor=1, center = False, scale = False):\n",
    "\n",
    "        label = self.label\n",
    "\n",
    "        feature_train = (self.f_coauthor_tn, self.f_text_tn, self.f_year_tn, self.f_venue_tn)\n",
    "        feature_test = (self.f_coauthor_ts, self.f_text_ts, self.f_year_ts, self.f_venue_ts)\n",
    "\n",
    "        feature_train = np.concatenate(feature_train,axis=1)\n",
    "        feature_test = np.concatenate(feature_test,axis=1)\n",
    "\n",
    "        n_index = np.where(label[:, -1] == 1)[0]\n",
    "        p_index = np.where(label[:, -1] != 1)[0]\n",
    "        x_n = feature_train[n_index]\n",
    "        x_p = feature_train[p_index]\n",
    "        y_n = label[n_index]\n",
    "        y_p = label[p_index]\n",
    "        x_pos_new = np.repeat(x_p, extend_factor, axis=0)\n",
    "        y_pos_new = np.repeat(y_p, extend_factor, axis=0)\n",
    "        label = np.concatenate((y_pos_new, y_n), axis=0)\n",
    "        feature_train = np.concatenate((x_pos_new, x_n), axis=0)\n",
    "\n",
    "        id_shuffle = list(range(label.shape[0]))\n",
    "        random.shuffle(id_shuffle)\n",
    "        label = label[id_shuffle]\n",
    "        feature_train = feature_train[id_shuffle]\n",
    "\n",
    "        if center and scale:\n",
    "            scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "            feature_train = scaler.fit_transform(feature_train)\n",
    "            feature_test = scaler.fit_transform(feature_train)\n",
    "        elif center and not scale:\n",
    "            scaler = StandardScaler(with_mean=True, with_std=False)\n",
    "            feature_train = scaler.fit_transform(feature_train)\n",
    "            feature_test = scaler.fit_transform(feature_train)\n",
    "        elif not center and scale:\n",
    "            scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "            feature_train = scaler.fit_transform(feature_train)\n",
    "            feature_test = scaler.fit_transform(feature_train)\n",
    "\n",
    "        return label, feature_train, feature_test\n",
    "\n",
    "    # Representation learning methods of feature coauthor\n",
    "    def transform_coauthor(self, method):\n",
    "        if method == \"rescale\":\n",
    "            self.transform_coauthor_rescale()\n",
    "        elif method == \"pca\":\n",
    "            self.transform_coauthor_rescale()\n",
    "            self.transform_coauthor_pca()\n",
    "        elif method == \"pe\":\n",
    "            self.transform_coauthor_pe()\n",
    "        elif method == \"onehot\":\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Invalid method name. Transform by default method: one-hot\")\n",
    "\n",
    "    def transform_coauthor_rescale(self):\n",
    "\n",
    "        n_author = 101\n",
    "        threshold_author = 100\n",
    "\n",
    "        cooperation_list = []\n",
    "\n",
    "        for author in self.train_data[\"authors\"]:\n",
    "            author = np.array(author)\n",
    "            prolific_author = author[author < threshold_author]\n",
    "            co_author = author[author >= threshold_author]\n",
    "            if prolific_author.size != 0:\n",
    "                for i in co_author:\n",
    "                    if i not in cooperation_list:\n",
    "                        cooperation_list.append(i)\n",
    "\n",
    "        cooperation_list.sort()\n",
    "\n",
    "        cth_list_train = []\n",
    "\n",
    "        for authors in self.train_data[\"authors\"]:\n",
    "            authors = np.array(authors)\n",
    "            cth_id = authors[authors >= threshold_author]\n",
    "            cth_vector = np.zeros(6576)\n",
    "            if len(cth_id) != 0:\n",
    "                for j in cth_id:\n",
    "                    if j in cooperation_list:\n",
    "                        cth_vector[cooperation_list.index(j)] = 1\n",
    "                    else:\n",
    "                        cth_vector[-1] = 1\n",
    "            else:\n",
    "                cth_vector[-1] = 1\n",
    "            cth_list_train.append(cth_vector)\n",
    "\n",
    "        cth_list_train = np.array(cth_list_train)\n",
    "\n",
    "        self.f_coauthor_tn = cth_list_train\n",
    "\n",
    "        cth_list_test = []\n",
    "\n",
    "        for authors in self.test_data[\"coauthors\"]:\n",
    "            authors = np.array(authors)\n",
    "            cth_id = authors[authors >= 100]\n",
    "            cth_vector = np.zeros(6576)\n",
    "            if len(cth_id) != 0:\n",
    "                for j in cth_id:\n",
    "                    if j in cooperation_list:\n",
    "                        cth_vector[cooperation_list.index(j)] = 1\n",
    "                    else:\n",
    "                        cth_vector[-1] = 1\n",
    "            else:\n",
    "                cth_vector[-1] = 1\n",
    "            cth_list_test.append(cth_vector)\n",
    "\n",
    "        cth_list_test = np.array(cth_list_test)\n",
    "\n",
    "        self.f_coauthor_ts = cth_list_test\n",
    "\n",
    "    def transform_coauthor_pe(self):\n",
    "        seq_len = 21247\n",
    "        d = 500\n",
    "        n = 10000\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        c_vec = P\n",
    "\n",
    "        train_data_coauthor = []\n",
    "        for i in self.train_data[\"authors\"]:\n",
    "            for j in i:\n",
    "                if j < 100:\n",
    "                    i.remove(j)\n",
    "            if len(i) == 0:\n",
    "                train_data_coauthor.append([21146])\n",
    "            else:\n",
    "                train_data_coauthor.append(i)\n",
    "\n",
    "        test_data_coauthor = list(self.test_data[\"coauthors\"])\n",
    "\n",
    "        coauthor_train = []\n",
    "        for c in train_data_coauthor:\n",
    "            ca = c_vec[c]\n",
    "            coauthor_train.append(np.mean(ca, axis=0))\n",
    "        coauthor_train = np.array(coauthor_train)\n",
    "\n",
    "        self.f_coauthor_tn = coauthor_train\n",
    "\n",
    "        coauthor_test = []\n",
    "        for c in test_data_coauthor:\n",
    "            ca = c_vec[c]\n",
    "            coauthor_test.append(np.mean(ca, axis=0))\n",
    "        coauthor_test = np.array(coauthor_test)\n",
    "\n",
    "        self.f_coauthor_ts = coauthor_test\n",
    "\n",
    "    def transform_coauthor_pca(self):\n",
    "        feature = self.f_coauthor_tn\n",
    "\n",
    "        npc = round(feature.shape[1] / 2)\n",
    "        pca1 = PCA(n_components=npc)\n",
    "        pca1.fit(feature)\n",
    "        pca_ratio = pca1.explained_variance_ratio_\n",
    "        for a in range(len(pca_ratio)):\n",
    "            if sum(pca_ratio[0:a]) > 0.9:\n",
    "                npc = a\n",
    "                break\n",
    "        pca2 = PCA(n_components=npc+1)\n",
    "        pca2.fit(feature)\n",
    "        feature_pca = pca2.transform(feature)\n",
    "\n",
    "        self.f_coauthor_tn = feature_pca\n",
    "        self.f_coauthor_ts = pca2.transform(self.f_coauthor_ts)\n",
    "\n",
    "    # Representation learning methods of feature text\n",
    "    def transform_text(self, method):\n",
    "        if method == \"w2v\":\n",
    "            self.transform_text_w2v()\n",
    "        elif method == \"d2v\":\n",
    "            self.transform_text_d2v()\n",
    "        elif method == \"pe\":\n",
    "            self.transform_text_pe()\n",
    "        elif method == \"pca\":\n",
    "            self.transform_text_pca()\n",
    "        elif method == \"onehot\":\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Invalid method name. Transform by default method: one-hot\")\n",
    "\n",
    "    def transform_text_w2v(self):\n",
    "\n",
    "        titles_tn = list(self.train_data[\"title\"].copy())\n",
    "        abstracts_tn = list(self.train_data[\"abstract\"].copy())\n",
    "\n",
    "        sentences = []\n",
    "        for i in range(len(titles_tn)):\n",
    "            text = titles_tn[i] + abstracts_tn[i]\n",
    "            sentences.append(text)\n",
    "\n",
    "        n_vector = 100\n",
    "\n",
    "        model = Word2Vec(sentences=sentences, vector_size=n_vector, min_count=1)\n",
    "        words_vec = model.wv\n",
    "\n",
    "        text_list_tn = []\n",
    "        for i in range(len(titles_tn)):\n",
    "            text = titles_tn[i] + abstracts_tn[i]\n",
    "            text_vector = np.array(words_vec[text])\n",
    "            text_list_tn.append(np.mean(text_vector, axis=0))\n",
    "        text_list_tn = np.array(text_list_tn)\n",
    "\n",
    "        self.f_text_tn = text_list_tn\n",
    "\n",
    "        titles_ts = list(self.test_data[\"title\"].copy())\n",
    "        abstracts_ts = list(self.test_data[\"abstract\"].copy())\n",
    "\n",
    "        text_list_ts = []\n",
    "        for i in range(len(titles_ts)):\n",
    "            text = titles_ts[i] + abstracts_ts[i]\n",
    "            text_vector = np.array(words_vec[text])\n",
    "            text_list_ts.append(np.mean(text_vector, axis=0))\n",
    "        text_list_ts = np.array(text_list_ts)\n",
    "\n",
    "        self.f_text_ts = text_list_ts\n",
    "\n",
    "    def transform_text_d2v(self):\n",
    "\n",
    "        titles_tn = self.train_data[\"title\"]\n",
    "        abstracts_tn = self.train_data[\"abstract\"]\n",
    "\n",
    "        text_base = []\n",
    "        for i in range(len(titles_tn)):\n",
    "            text_base.append(titles_tn[i] + abstracts_tn[i])\n",
    "\n",
    "        document = [TaggedDocument(doc, [i]) for i, doc in enumerate(text_base)]\n",
    "\n",
    "        n_vector = 100\n",
    "\n",
    "        model = Doc2Vec(document, vector_size=n_vector, window=2, min_count=1)\n",
    "        words_vector = []\n",
    "        for i in range(len(text_base)):\n",
    "            words_vector.append(list(model.infer_vector(np.asarray(text_base[i], dtype=\"str\"))))\n",
    "        words_vector = np.array(words_vector)\n",
    "\n",
    "        text_list_tn = []\n",
    "        for i in range(len(titles_tn)):\n",
    "            text = titles_tn[i] + abstracts_tn[i]\n",
    "            text_vector = np.array(words_vector[text])\n",
    "            text_list_tn.append(np.mean(text_vector, axis=0))\n",
    "        text_list_tn = np.array(text_list_tn)\n",
    "\n",
    "        self.f_text_tn = text_list_tn\n",
    "\n",
    "        titles_ts = self.test_data[\"title\"]\n",
    "        abstracts_ts = self.test_data[\"abstract\"]\n",
    "\n",
    "        text_list_ts = []\n",
    "        for i in range(len(titles_ts)):\n",
    "            text = titles_ts[i] + abstracts_ts[i]\n",
    "            text_vector = np.array(words_vector[text])\n",
    "            text_list_ts.append(np.mean(text_vector, axis=0))\n",
    "        text_list_ts = np.array(text_list_ts)\n",
    "\n",
    "        self.f_text_ts = text_list_ts\n",
    "\n",
    "    def transform_text_pe(self):\n",
    "        seq_len = 5000\n",
    "        d = 100\n",
    "        n = 10000\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        t_vec = P\n",
    "\n",
    "        train_data_text = []\n",
    "        for i in range(len(self.train_data)):\n",
    "            train_data_text.append(list(self.train_data[\"title\"][i]) + list(self.train_data[\"abstract\"][i]))\n",
    "\n",
    "        test_data_text = []\n",
    "        for i in range(len(self.test_data)):\n",
    "            test_data_text.append(list(self.test_data[\"title\"][i]) + list(self.test_data[\"abstract\"][i]))\n",
    "\n",
    "        text_train = []\n",
    "        for t in train_data_text:\n",
    "            tx = t_vec[t]\n",
    "            text_train.append(np.mean(tx, axis=0))\n",
    "        text_train = np.array(text_train)\n",
    "\n",
    "\n",
    "        self.f_text_tn = text_train\n",
    "\n",
    "        text_test = []\n",
    "        for t in test_data_text:\n",
    "            tx = t_vec[t]\n",
    "            text_test.append(np.mean(tx, axis=0))\n",
    "        text_test = np.array(text_test)\n",
    "\n",
    "        self.f_text_ts = text_test\n",
    "\n",
    "    def transform_text_pca(self):\n",
    "        feature = self.f_text_tn\n",
    "\n",
    "        npc = round(feature.shape[1] / 2)\n",
    "        pca1 = PCA(n_components=npc)\n",
    "        pca1.fit(feature)\n",
    "        pca_ratio = pca1.explained_variance_ratio_\n",
    "        for a in range(len(pca_ratio)):\n",
    "            if sum(pca_ratio[0:a]) > 0.9:\n",
    "                npc = a\n",
    "                break\n",
    "        pca2 = PCA(n_components=npc+1)\n",
    "        pca2.fit(feature)\n",
    "        feature_pca = pca2.transform(feature)\n",
    "\n",
    "        self.f_text_tn = feature_pca\n",
    "        self.f_text_ts = pca2.transform(self.f_text_ts)\n",
    "\n",
    "    # Representation learning methods of feature venue\n",
    "    def transform_venue(self, method):\n",
    "        if method == \"pe\":\n",
    "            self.transform_venue_pe()\n",
    "        elif method == \"pca\":\n",
    "            self.transform_venue_pca()\n",
    "        elif method == \"onehot\":\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Invalid method name. Transform by default method: one-hot\")\n",
    "\n",
    "    def transform_venue_pe(self):\n",
    "\n",
    "        seq_len = 466\n",
    "        d = 2\n",
    "        n = 10000\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        v_vec = P\n",
    "\n",
    "        venue_train = []\n",
    "        for v in self.train_data[\"venue\"]:\n",
    "            if v == \"\":\n",
    "                vn = v_vec[-1]\n",
    "            else:\n",
    "                vn = v_vec[v]\n",
    "            venue_train.append(vn)\n",
    "        venue_train = np.array(venue_train)\n",
    "\n",
    "        self.f_venue_tn = venue_train\n",
    "\n",
    "        venue_test = []\n",
    "        for v in self.test_data[\"venue\"]:\n",
    "            if v == \"\":\n",
    "                vn = v_vec[-1]\n",
    "            else:\n",
    "                vn = v_vec[v]\n",
    "            venue_test.append(vn)\n",
    "        venue_test = np.array(venue_test)\n",
    "\n",
    "        self.f_venue_ts = venue_test\n",
    "\n",
    "    def transform_venue_pca(self):\n",
    "\n",
    "        feature = self.f_venue_tn\n",
    "\n",
    "        npc = round(feature.shape[1] / 2)\n",
    "        pca1 = PCA(n_components=npc)\n",
    "        pca1.fit(feature)\n",
    "        pca_ratio = pca1.explained_variance_ratio_\n",
    "        for a in range(len(pca_ratio)):\n",
    "            if sum(pca_ratio[0:a]) > 0.9:\n",
    "                npc = a\n",
    "                break\n",
    "        pca2 = PCA(n_components=npc+1)\n",
    "        pca2.fit(feature)\n",
    "\n",
    "        self.f_venue_tn = pca2.transform(self.f_venue_tn)\n",
    "        self.f_venue_ts = pca2.transform(self.f_venue_ts)\n",
    "\n",
    "    # Representation learning methods of feature year\n",
    "    def transform_year(self, method):\n",
    "        if method == \"pe\":\n",
    "            self.transform_year_pe()\n",
    "        elif method == \"pca\":\n",
    "            self.transform_year_pca()\n",
    "        elif method == \"numeric\":\n",
    "            self.transform_year_numeric()\n",
    "        elif method == \"onehot\":\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Invalid method name. Transform by default method: one-hot\")\n",
    "\n",
    "    def transform_year_numeric(self):\n",
    "        self.f_year_tn = np.array(self.train_data[\"year\"]).reshape(len(self.train_data[\"year\"]),1)\n",
    "        self.f_year_ts = np.array(self.test_data[\"year\"]).reshape(len(self.test_data[\"year\"]),1)\n",
    "\n",
    "    def transform_year_pe(self):\n",
    "\n",
    "        seq_len = 20\n",
    "        d = 10\n",
    "        n = 10000\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        y_vec = P\n",
    "\n",
    "        year_train = []\n",
    "        for y in self.train_data[\"year\"]:\n",
    "            yr = y_vec[y]\n",
    "            year_train.append(yr)\n",
    "        year_train = np.array(year_train)\n",
    "\n",
    "        self.f_year_tn = year_train\n",
    "\n",
    "        year_test = []\n",
    "        for y in self.test_data[\"year\"]:\n",
    "            if y == \"\":\n",
    "                yr = y_vec[-1]\n",
    "            else:\n",
    "                yr = y_vec[y]\n",
    "            year_test.append(yr)\n",
    "        year_test = np.array(year_test)\n",
    "\n",
    "        self.f_year_ts = year_test\n",
    "\n",
    "    def transform_year_pca(self):\n",
    "\n",
    "        feature = self.f_year_tn\n",
    "\n",
    "        npc = round(feature.shape[1] / 2)\n",
    "        pca1 = PCA(n_components=npc)\n",
    "        pca1.fit(feature)\n",
    "        pca_ratio = pca1.explained_variance_ratio_\n",
    "        for a in range(len(pca_ratio)):\n",
    "            if sum(pca_ratio[0:a]) > 0.9:\n",
    "                npc = a\n",
    "                break\n",
    "        pca2 = PCA(n_components=npc+1)\n",
    "        pca2.fit(feature)\n",
    "\n",
    "        self.f_year_tn = pca2.transform(self.f_year_tn)\n",
    "        self.f_year_ts = pca2.transform(self.f_year_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Processor Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "authorship_data = AuthorshipData(train_file=\"Data/train.json\", test_file=\"Data/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Representation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Transform feature coauthor: Choose one from \"onehot\",  \"pe\", \"pca\", or \"rescale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "authorship_data.transform_coauthor(method=\"pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Transform feature text: Choose one from \"onehot\", \"w2v\", \"d2v\", \"pe\", or \"pca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "authorship_data.transform_text(method=\"w2v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Transform feature year: Choose one from \"onehot\", \"pe\", \"pca\", or \"numeric\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "authorship_data.transform_year(method=\"numeric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Transform feature venue: Choose one from \"onehot\", \"pe\" or \"pca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "authorship_data.transform_venue(method=\"pe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extract & Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_train, x_train, x_test = authorship_data.extract_data(extend_factor = 2, center = True, scale = True)\n",
    "np.save(\"y_train_new.npy\", y_train)\n",
    "np.save(\"x_train_new.npy\", x_train)\n",
    "np.save(\"x_test_new.npy\", x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
