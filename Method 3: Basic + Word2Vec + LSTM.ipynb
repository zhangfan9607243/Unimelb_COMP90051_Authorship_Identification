{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load original data in pandas\n",
    "data_tran = pd.read_json('data2/data_tran.json', orient='records', lines=True)\n",
    "data_test = pd.read_json('data2/data_test.json', orient='records', lines=True)\n",
    "\n",
    "n_tran = data_tran.shape[0]\n",
    "n_test = data_test.shape[0]\n",
    "\n",
    "# Load feature data in numpy\n",
    "x_tran_coauthors = np.load(\"data2/x_tran_coauthors.npy\")\n",
    "x_tran_venue_a   = np.load(\"data2/x_tran_venue_a.npy\")\n",
    "x_tran_venue_b   = np.load(\"data2/x_tran_venue_b.npy\")\n",
    "x_tran_text_a    = np.load(\"data2/x_tran_text_a.npy\")\n",
    "x_tran_text_b    = np.load(\"data2/x_tran_text_b.npy\")\n",
    "\n",
    "x_test_coauthors = np.load(\"data2/x_test_coauthors.npy\")\n",
    "x_test_venue_a   = np.load(\"data2/x_test_venue_a.npy\")\n",
    "x_test_venue_b   = np.load(\"data2/x_test_venue_b.npy\")\n",
    "x_test_text_a    = np.load(\"data2/x_test_text_a.npy\")\n",
    "x_test_text_b    = np.load(\"data2/x_test_text_b.npy\")\n",
    "\n",
    "x_tran_title_doc2vec = np.load('data2/x_tran_title_doc2vec.npy')\n",
    "x_test_title_doc2vec = np.load('data2/x_test_title_doc2vec.npy')\n",
    "\n",
    "x_tran_abstract_doc2vec = np.load('data2/x_tran_abstract_doc2vec.npy')\n",
    "x_test_abstract_doc2vec = np.load('data2/x_test_abstract_doc2vec.npy')\n",
    "\n",
    "y_tran_basic = np.load(\"data2/y_tran.npy\")\n",
    "x_tran_basic = np.concatenate((x_tran_coauthors, x_tran_venue_a, x_tran_venue_b, x_tran_text_a, x_tran_text_b), axis=1)\n",
    "x_test_basic = np.concatenate((x_test_coauthors, x_test_venue_a, x_test_venue_b, x_test_text_a, x_test_text_b), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Train-validation split\n",
    "idxs = np.random.permutation(n_tran)\n",
    "inxs_prop = int(0.85 * n_tran)\n",
    "idxs_tran_indices = idxs[:inxs_prop]\n",
    "idxs_vald_indices = idxs[inxs_prop:]\n",
    "\n",
    "x_tran_a = torch.tensor(x_tran_basic[idxs_tran_indices], dtype=torch.float32).to(device)\n",
    "x_vald_a = torch.tensor(x_tran_basic[idxs_vald_indices], dtype=torch.float32).to(device)\n",
    "x_test_a = torch.tensor(x_test_basic, dtype=torch.float32).to(device)\n",
    "\n",
    "y_tran   = torch.tensor(y_tran_basic[idxs_tran_indices], dtype=torch.float32).to(device)\n",
    "y_vald   = torch.tensor(y_tran_basic[idxs_vald_indices], dtype=torch.float32).to(device)\n",
    "y_test   = torch.zeros((x_test_a.shape[0], y_tran.shape[1]), dtype=torch.float32)\n",
    "\n",
    "data_tran_new = data_tran.iloc[idxs_tran_indices].reset_index(drop=True)\n",
    "data_vald_new = data_tran.iloc[idxs_vald_indices].reset_index(drop=True)\n",
    "data_test_new = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data2/x_tran_title_word_vectors.json\", \"r\") as f:\n",
    "    word_vectors_dict_title = json.load(f)\n",
    "\n",
    "with open(\"data2/x_tran_abstract_word_vectors.json\", \"r\") as f:\n",
    "    word_vectors_dict_abstract = json.load(f)\n",
    "\n",
    "word_vectors_dict_title = {word: np.array(vector) for word, vector in word_vectors_dict_title.items()}\n",
    "word_vectors_dict_abstract = {word: np.array(vector) for word, vector in word_vectors_dict_abstract.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class ModelDataset(Dataset):\n",
    "    def __init__(self, x1, y, data, word_vectors_dict_title, word_vectors_dict_abstract):\n",
    "        self.x1 = x1\n",
    "        self.y  = y\n",
    "        self.data = data\n",
    "        self.word_vectors_dict_title    = word_vectors_dict_title\n",
    "        self.word_vectors_dict_abstract = word_vectors_dict_abstract\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def text_to_vector(self, text, word_vectors_dict):\n",
    "        vectors = [word_vectors_dict.get(word, np.zeros_like(next(iter(word_vectors_dict.values())))) for word in text.split()]\n",
    "        return torch.tensor(np.stack(vectors), dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x1 = self.x1[idx].clone().detach().float()\n",
    "        x2 = self.text_to_vector(self.data.iloc[idx]['title_text'],    self.word_vectors_dict_title).to(device)\n",
    "        x3 = self.text_to_vector(self.data.iloc[idx]['abstract_text'], self.word_vectors_dict_abstract).to(device)\n",
    "\n",
    "        y = self.y[idx].clone().detach().float()\n",
    "        \n",
    "        return x1, x2, x3, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 解包每个 batch 元素：x1, x2, x3, y\n",
    "    x1_batch, x2_batch, x3_batch, y_batch = zip(*batch)\n",
    "\n",
    "    # 将 x1 转为张量 (基础特征)\n",
    "    x1_batch = torch.stack(x1_batch)\n",
    "\n",
    "    # 使用 pad_sequence 填充 x2 和 x3\n",
    "    x2_batch = pad_sequence(x2_batch, batch_first=True)\n",
    "    x3_batch = pad_sequence(x3_batch, batch_first=True)\n",
    "\n",
    "    # 计算 x2 和 x3 的长度\n",
    "    x2_lengths = torch.tensor([len(seq) for seq in x2_batch])\n",
    "    x3_lengths = torch.tensor([len(seq) for seq in x3_batch])\n",
    "\n",
    "    # 将 y 转为张量\n",
    "    y_batch = torch.stack(y_batch)\n",
    "\n",
    "    return x1_batch, x2_batch, x3_batch, x2_lengths, x3_lengths, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tran = ModelDataset(x_tran_a, y_tran, data_tran_new, word_vectors_dict_title, word_vectors_dict_abstract)\n",
    "dataset_vald = ModelDataset(x_vald_a, y_vald, data_vald_new, word_vectors_dict_title, word_vectors_dict_abstract)\n",
    "dataset_test = ModelDataset(x_test_a, y_test, data_test_new, word_vectors_dict_title, word_vectors_dict_abstract)\n",
    "\n",
    "datalod_tran = DataLoader(dataset_tran, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "datalod_vald = DataLoader(dataset_vald, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "datalod_test = DataLoader(dataset_test, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "class FNNLSTM(nn.Module):\n",
    "    def __init__(self, input_dim1, output_dim):\n",
    "        super(FNNLSTM, self).__init__()\n",
    "\n",
    "        # 定义 model1 的全连接层\n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Linear(input_dim1, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 50),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 定义 LSTM 层\n",
    "        self.lstm_title = nn.LSTM(input_size=50, hidden_size=50, batch_first=True)\n",
    "        self.lstm_abstract = nn.LSTM(input_size=100, hidden_size=100, batch_first=True)\n",
    "\n",
    "        # 定义输出层\n",
    "        self.fc = nn.Linear(200, output_dim)  # 50 + 100 + 50 = 200\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x1, x2, x2_lengths, x3, x3_lengths):\n",
    "        # model1 前向传播\n",
    "        x1_out = self.model1(x1)  # shape: (batch_size, 50)\n",
    "\n",
    "        # 对 x2 进行 pack，并通过 LSTM\n",
    "        x2_packed = rnn_utils.pack_padded_sequence(x2, x2_lengths, batch_first=True, enforce_sorted=False)\n",
    "        x2_out, (x2_hidden, _) = self.lstm_title(x2_packed)\n",
    "        x2_hidden = x2_hidden[-1]  # 获取最后一层的 hidden state\n",
    "\n",
    "        # 对 x3 进行 pack，并通过 LSTM\n",
    "        x3_packed = rnn_utils.pack_padded_sequence(x3, x3_lengths, batch_first=True, enforce_sorted=False)\n",
    "        x3_out, (x3_hidden, _) = self.lstm_abstract(x3_packed)\n",
    "        x3_hidden = x3_hidden[-1]  # 获取最后一层的 hidden state\n",
    "\n",
    "        # 拼接所有特征\n",
    "        x = torch.cat([x1_out, x2_hidden, x3_hidden], dim=1)  # shape: (batch_size, 200)\n",
    "\n",
    "        # 通过全连接层\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim1 = x_tran_a.shape[1]\n",
    "output_dim = y_tran.shape[1] \n",
    "\n",
    "model = FNNLSTM(input_dim1, output_dim).to(device)\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(pred_label, true_label):\n",
    "    pred_label = pred_label.int()\n",
    "    true_label = true_label.int()\n",
    "    pc = precision_score(true_label.cpu(), pred_label.cpu(), average='macro', zero_division=0)\n",
    "    rc = recall_score(true_label.cpu(), pred_label.cpu(), average='macro', zero_division=0)\n",
    "    f1 = f1_score(true_label.cpu(), pred_label.cpu(), average='macro', zero_division=0)\n",
    "    return pc, rc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss):\n",
    "        if self.best_loss is None or train_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = train_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10, delta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[20000, 1]' is invalid for input of size 10000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m x1, x2, x3, y \u001b[38;5;241m=\u001b[39m x1\u001b[38;5;241m.\u001b[39mto(device), x2\u001b[38;5;241m.\u001b[39mto(device), x3\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[1;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(x1, x2, x3)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[83], line 37\u001b[0m, in \u001b[0;36mFNNLSTM.forward\u001b[0;34m(self, x1, x2, x2_lengths, x3, x3_lengths)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 对 x2 进行 pack，并通过 LSTM\u001b[39;00m\n\u001b[1;32m     36\u001b[0m x2_packed \u001b[38;5;241m=\u001b[39m rnn_utils\u001b[38;5;241m.\u001b[39mpack_padded_sequence(x2, x2_lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 37\u001b[0m x2_out, (x2_hidden, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_title\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2_packed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m x2_hidden \u001b[38;5;241m=\u001b[39m x2_hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# 获取最后一层的 hidden state\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 对 x3 进行 pack，并通过 LSTM\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:914\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    911\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    912\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 914\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    917\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[20000, 1]' is invalid for input of size 10000"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "epochs = 50000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    total_loss = 0 \n",
    "\n",
    "    for batch in datalod_tran:\n",
    "        x1, x2, x3, x2_lengths, x3_lengths, y = batch\n",
    "        x1, x2, x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(x1, x2, x2_lengths, x3, x3_lengths)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(x1, x2, x3)\n",
    "        loss = criterion(outputs, y.float())\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(datalod_tran)  # 计算平均损失\n",
    "\n",
    "    # 每 100 个 epoch 打印训练和验证结果\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        model.eval()  # 设置为评估模式\n",
    "\n",
    "        y_tran_pred_prob, y_tran_pred_labl = [], []\n",
    "        y_vald_pred_prob, y_vald_pred_labl = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 遍历训练数据集\n",
    "            for batch in datalod_tran:\n",
    "                x1_tran, x2_tran, x3_tran, y_tran_batch = batch\n",
    "                x1_tran, x2_tran, x3_tran, y_tran_batch = (\n",
    "                    x1_tran.to(device), x2_tran.to(device), \n",
    "                    x3_tran.to(device), y_tran_batch.to(device)\n",
    "                )\n",
    "\n",
    "                tran_outputs = model(x1_tran, x2_tran, x3_tran)\n",
    "                y_tran_pred_prob.append(tran_outputs.cpu())\n",
    "                y_tran_pred_labl.append(y_tran_batch.cpu())\n",
    "\n",
    "            # 遍历验证数据集\n",
    "            for batch in datalod_vald:\n",
    "                x1_vald, x2_vald, x3_vald, y_vald_batch = batch\n",
    "                x1_vald, x2_vald, x3_vald, y_vald_batch = (\n",
    "                    x1_vald.to(device), x2_vald.to(device), \n",
    "                    x3_vald.to(device), y_vald_batch.to(device)\n",
    "                )\n",
    "\n",
    "                vald_outputs = model(x1_vald, x2_vald, x3_vald)\n",
    "                y_vald_pred_prob.append(vald_outputs.cpu())\n",
    "                y_vald_pred_labl.append(y_vald_batch.cpu())\n",
    "\n",
    "            # 合并所有批次的结果\n",
    "            y_tran_pred_prob = torch.cat(y_tran_pred_prob, dim=0)\n",
    "            y_tran_pred_labl = torch.cat(y_tran_pred_labl, dim=0)\n",
    "            y_vald_pred_prob = torch.cat(y_vald_pred_prob, dim=0)\n",
    "            y_vald_pred_labl = torch.cat(y_vald_pred_labl, dim=0)\n",
    "\n",
    "            # 计算指标\n",
    "            tran_pc, tran_rc, tran_f1 = calculate_metrics(\n",
    "                (y_tran_pred_prob > 0.5).int(), y_tran_pred_labl\n",
    "            )\n",
    "            vald_pc, vald_rc, vald_f1 = calculate_metrics(\n",
    "                (y_vald_pred_prob > 0.5).int(), y_vald_pred_labl\n",
    "            )\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Train - Precision: {tran_pc:.4f}, Recall: {tran_rc:.4f}, F1 Score: {tran_f1:.4f}\")\n",
    "        print(f\"Val   - Precision: {vald_pc:.4f}, Recall: {vald_rc:.4f}, F1 Score: {vald_f1:.4f}\")\n",
    "        print()\n",
    "\n",
    "        early_stopping(avg_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early Stop!\")\n",
    "            break\n",
    "\n",
    "# 测试集预测\n",
    "model.eval()\n",
    "y_test_pred_prob, y_test_pred_labl = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in datalod_test:\n",
    "        x1_test, x2_test, x3_test, _ = batch  # 测试集无标签\n",
    "        x1_test, x2_test, x3_test = (\n",
    "            x1_test.to(device), x2_test.to(device), x3_test.to(device)\n",
    "        )\n",
    "\n",
    "        test_outputs = model(x1_test, x2_test, x3_test)\n",
    "        y_test_pred_prob.append(test_outputs.cpu())\n",
    "\n",
    "    # 合并所有批次的结果\n",
    "    y_test_pred_prob = torch.cat(y_test_pred_prob, dim=0)\n",
    "    y_test_pred_labl = (y_test_pred_prob > 0.5).int()\n",
    "\n",
    "# 打印测试集预测结果的形状\n",
    "print(f\"Test Prediction Shape: {y_test_pred_labl.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([484, 484, 484, 484, 484, 484, 484, 484, 484, 484, 484, 484, 484, 484,\n",
       "        484, 484, 484, 484, 484, 484, 484, 484, 484, 484, 484, 484, 484, 484,\n",
       "        484, 484, 484, 484])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_csv(x_test_a, y_test_pred_labl):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for i, row in enumerate(y_test_pred_labl):\n",
    "        if ((x_test_a[i, :100] < 1).all() or (x_test_a[i, 100:200] == 0).all() or (x_test_a[i, 200:300] == 0).all() or (x_test_a[i, 300:400] == 0).all() or (x_test_a[i, 400:500] == 0).all()):\n",
    "            result.append(\"-1\")\n",
    "        elif row.sum() == 0 or row[100] == 1:\n",
    "            result.append(\"-1\")\n",
    "        else:\n",
    "            indices = [str(idx) for idx, val in enumerate(row) if val == 1]\n",
    "            result.append(\" \".join(indices))\n",
    "    \n",
    "    result_df = pd.DataFrame({\"ID\": range(len(result)), \"Predict\": result})\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "generate_output_csv(x_test_a, y_test_pred_labl).to_csv(\"result_method2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
